{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T15:04:49.104493Z","iopub.status.busy":"2024-06-05T15:04:49.104111Z","iopub.status.idle":"2024-06-05T15:05:06.678952Z","shell.execute_reply":"2024-06-05T15:05:06.678135Z","shell.execute_reply.started":"2024-06-05T15:04:49.104453Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\ADMIN\\miniconda3\\envs\\dnn\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, Dataset\n","from transformers import BertTokenizer, BertModel\n","import torch.optim as optim\n","from torch.nn.utils import clip_grad_norm_\n","from tqdm import tqdm\n","import os\n","from jiwer import wer\n","import yaml\n","import json "]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["####### Config #######\n","config_path = \"conf_1\"\n","config_file = os.path.join(config_path, \"config.yml\")\n","with open(config_file,'r') as conf:\n","    config = yaml.load(conf, Loader=yaml.SafeLoader)\n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T15:05:06.682657Z","iopub.status.busy":"2024-06-05T15:05:06.681829Z","iopub.status.idle":"2024-06-05T15:05:06.688855Z","shell.execute_reply":"2024-06-05T15:05:06.687794Z","shell.execute_reply.started":"2024-06-05T15:05:06.682598Z"},"trusted":true},"outputs":[],"source":["class AddNorm(nn.Module):\n","    def __init__(self, norm_shape: int, dropout=0.2):\n","        super().__init__()\n","        self.dropout = nn.Dropout(dropout)\n","        self.ln = nn.LayerNorm(norm_shape)\n","\n","    def forward(self, X, Y):\n","        return self.ln(self.dropout(Y) + X)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T15:05:06.690593Z","iopub.status.busy":"2024-06-05T15:05:06.690234Z","iopub.status.idle":"2024-06-05T15:05:06.713966Z","shell.execute_reply":"2024-06-05T15:05:06.712899Z","shell.execute_reply.started":"2024-06-05T15:05:06.690560Z"},"trusted":true},"outputs":[],"source":["class FeedForwardNetwork(nn.Module):\n","    def __init__(self, input_dim: int, hidden_ff_dim: int, dropout=0.2):\n","        super().__init__()\n","        self.linear1 = nn.Linear(input_dim, hidden_ff_dim)\n","        self.dropout = nn.Dropout(dropout)\n","        self.relu1 = nn.ReLU()\n","        self.linear2 = nn.Linear(hidden_ff_dim, input_dim)\n","\n","    def forward(self, x):\n","        return self.linear2(self.dropout(self.relu1(self.linear1(x))))"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T15:05:06.715507Z","iopub.status.busy":"2024-06-05T15:05:06.715236Z","iopub.status.idle":"2024-06-05T15:05:06.723208Z","shell.execute_reply":"2024-06-05T15:05:06.722149Z","shell.execute_reply.started":"2024-06-05T15:05:06.715481Z"},"trusted":true},"outputs":[],"source":["class ShrinkNorm(nn.Module):\n","    def __init__(self, input_dim: int, shrink_norm_hidden: int, output_dim: int, dropout=0.2):\n","        super().__init__()\n","        self.dropout = nn.Dropout(dropout)\n","        self.linear1 = nn.Linear(input_dim, shrink_norm_hidden)\n","        self.relu1 = nn.ReLU()\n","        self.linear2 = nn.Linear(shrink_norm_hidden, output_dim)\n","        self.ln = nn.LayerNorm(output_dim)\n","\n","    def forward(self, x):\n","        return self.ln(self.linear2(self.dropout(self.relu1(self.linear1(x)))))"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T15:05:06.724796Z","iopub.status.busy":"2024-06-05T15:05:06.724508Z","iopub.status.idle":"2024-06-05T15:05:06.740413Z","shell.execute_reply":"2024-06-05T15:05:06.739466Z","shell.execute_reply.started":"2024-06-05T15:05:06.724767Z"},"trusted":true},"outputs":[],"source":["class SinoVietnameseTranslator(nn.Module):\n","    def __init__(self, tokenizer, base_model, vocab, hidden_ff_dim=512, model_hidden_dim=512, \n","                 large_hidden_classification_head_dim=256, small_hidden_classification_head_dim=128,\n","                 shrink_norm_hidden=512, max_num_spellings=7, num_spelling_threshold=3, train_bert_param=True, dropout=0.2):\n","        super(SinoVietnameseTranslator, self).__init__()\n","        self.tokenizer = tokenizer\n","        self.bert = base_model\n","        self.vocab = vocab\n","        self.max_num_spellings = max_num_spellings\n","        \n","        for param in self.bert.parameters():\n","            param.requires_grad = train_bert_param\n","        \n","        self.shrink_norm = ShrinkNorm(self.bert.config.hidden_size,shrink_norm_hidden, model_hidden_dim, dropout)\n","        self.feed_forward = FeedForwardNetwork(model_hidden_dim, hidden_ff_dim, dropout)\n","        self.add_norm = AddNorm(model_hidden_dim, dropout)\n","        \n","        self.classification_heads = nn.ModuleDict()\n","        for sino_word, viet_spellings in self.vocab.items():\n","            if len(viet_spellings) > 1 and len(viet_spellings) <= num_spelling_threshold:\n","                num_spellings = len(viet_spellings)\n","                self.classification_heads[sino_word] = nn.Sequential(\n","                    nn.Linear(model_hidden_dim, small_hidden_classification_head_dim),\n","                    nn.ReLU(),\n","                    nn.Dropout(dropout),\n","                    nn.Linear(small_hidden_classification_head_dim, num_spellings),\n","                    nn.Softmax(dim=-1)\n","                )\n","            elif len(viet_spellings) > num_spelling_threshold:\n","                num_spellings = len(viet_spellings)\n","                self.classification_heads[sino_word] = nn.Sequential(\n","                    nn.Linear(model_hidden_dim, large_hidden_classification_head_dim),\n","                    nn.ReLU(),\n","                    nn.Dropout(dropout),\n","                    nn.Linear(large_hidden_classification_head_dim, num_spellings),\n","                    nn.Softmax(dim=-1)\n","                )\n","\n","    def forward(self, input_ids, attention_mask=None):\n","        outputs = self.bert(input_ids, attention_mask=attention_mask)\n","        sequence_output = outputs.last_hidden_state\n","        shrink_output = self.shrink_norm(sequence_output)\n","        projected_output = self.add_norm(shrink_output, self.feed_forward(shrink_output))\n","        \n","        batch_size, max_len = input_ids.size()\n","        predictions = torch.full((batch_size, max_len, self.max_num_spellings), -1.0, device=input_ids.device)\n","        \n","        for i in range(batch_size):\n","            for j in range(max_len):\n","                token_id = input_ids[i, j].item()\n","                if token_id == self.tokenizer.pad_token_id:\n","                    continue\n","                    \n","                sino_word = self.tokenizer.convert_ids_to_tokens(token_id)\n","                \n","                if sino_word in self.classification_heads:\n","                    logits = self.classification_heads[sino_word](projected_output[i, j])\n","                    predictions[i, j, :len(logits)] = logits\n","                else:\n","                    predictions[i, j, 0] = 1.0\n","\n","        return predictions"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T15:05:06.742065Z","iopub.status.busy":"2024-06-05T15:05:06.741648Z","iopub.status.idle":"2024-06-05T15:05:06.753451Z","shell.execute_reply":"2024-06-05T15:05:06.752504Z","shell.execute_reply.started":"2024-06-05T15:05:06.742031Z"},"trusted":true},"outputs":[],"source":["class SinoVietnameseDataset(Dataset):\n","    def __init__(self, tokenizer, data, vocab, max_len=512):\n","        self.data = data\n","        self.tokenizer = tokenizer\n","        self.vocab = vocab\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        sino_sent, viet_sent = self.data[idx]\n","        sino_tokens = self.tokenizer.encode(sino_sent, add_special_tokens=False, max_length=self.max_len, truncation=True)\n","        viet_spellings = viet_sent.split()\n","\n","        input_ids = sino_tokens + [self.tokenizer.pad_token_id] * (self.max_len - len(sino_tokens))\n","\n","        labels = []\n","        for i, sino_word_id in enumerate(sino_tokens):\n","            sino_word = self.tokenizer.convert_ids_to_tokens(sino_word_id)\n","            if sino_word in self.vocab:\n","                viet_spellings_for_word = self.vocab[sino_word]\n","                if len(viet_spellings_for_word) > 1:\n","                    label = viet_spellings_for_word.index(viet_spellings[i])\n","                else:\n","                    label = -1\n","            else:\n","                label = -1\n","            labels.append(label)\n","\n","        labels += [-1] * (self.max_len - len(labels))  # Padding\n","        attention_mask = [1] * len(sino_tokens) + [0] * (self.max_len - len(sino_tokens))\n","\n","        return {\n","            \"input_ids\": torch.tensor(input_ids),\n","            \"labels\": torch.tensor(labels),\n","            \"attention_mask\": torch.tensor(attention_mask),\n","        }\n"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T15:05:06.755156Z","iopub.status.busy":"2024-06-05T15:05:06.754777Z","iopub.status.idle":"2024-06-05T15:05:11.645685Z","shell.execute_reply":"2024-06-05T15:05:11.644684Z","shell.execute_reply.started":"2024-06-05T15:05:06.755131Z"},"trusted":true},"outputs":[],"source":["\n","def load_data(data_file):\n","    data = []\n","    with open(data_file, 'r', encoding='utf-8') as f:\n","        for line in f.readlines():\n","            if ',' not in line:\n","                continue\n","            sino_sent, viet_sent = line.strip().split(',')\n","            data.append((sino_sent, viet_sent))\n","    return data\n","\n","train_data_path = \"data/train.txt\"\n","test_data_path = \"data/test.txt\"\n","train_data = load_data(train_data_path)\n","test_data = load_data(test_data_path)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'dict'>\n","7688\n","<class 'list'>\n","7688\n"]}],"source":["with open('vocab/vocab.json', 'r') as vocab_file, open('vocab/sino_viet_words.json', 'r') as words_file:\n","    base_vocab = json.load(vocab_file)\n","    sino_viet_words = json.load(words_file)\n","\n","print(type(base_vocab))\n","print(len(base_vocab))\n","print(type(sino_viet_words))\n","print(len(sino_viet_words))"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T15:05:11.648555Z","iopub.status.busy":"2024-06-05T15:05:11.648226Z","iopub.status.idle":"2024-06-05T15:05:19.112721Z","shell.execute_reply":"2024-06-05T15:05:19.111756Z","shell.execute_reply.started":"2024-06-05T15:05:11.648529Z"},"trusted":true},"outputs":[{"data":{"text/plain":["Embedding(23683, 768)"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["# Model Config\n","bert_model = config['model_config']['bert_model'] \n","\n","base_tokenizer = BertTokenizer.from_pretrained(bert_model)\n","base_tokenizer.add_tokens(sino_viet_words)\n","\n","base_model = BertModel.from_pretrained(bert_model)\n","base_model.resize_token_embeddings(len(base_tokenizer))"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T15:05:19.114242Z","iopub.status.busy":"2024-06-05T15:05:19.113926Z","iopub.status.idle":"2024-06-05T15:05:19.121386Z","shell.execute_reply":"2024-06-05T15:05:19.120338Z","shell.execute_reply.started":"2024-06-05T15:05:19.114215Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Train set: 153372\n","Test set: 17042\n","Train batch num: 4793\n","Test batch num: 533\n"]}],"source":["# Data Config\n","batch_size = config['data_config']['batch_size']\n","max_len = config['data_config']['max_len']\n","\n","train_dataset = SinoVietnameseDataset(base_tokenizer, train_data, base_vocab, max_len)\n","test_dataset = SinoVietnameseDataset(base_tokenizer, test_data, base_vocab, max_len)\n","\n","print(f\"Train set: {len(train_dataset)}\")\n","print(f\"Test set: {len(test_dataset)}\")\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","print(f\"Train batch num: {len(train_loader)}\")\n","print(f\"Test batch num: {len(test_loader)}\")"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T15:05:19.123197Z","iopub.status.busy":"2024-06-05T15:05:19.122826Z","iopub.status.idle":"2024-06-05T15:05:19.322716Z","shell.execute_reply":"2024-06-05T15:05:19.321808Z","shell.execute_reply.started":"2024-06-05T15:05:19.123163Z"},"trusted":true},"outputs":[],"source":["def decode_predictions(predictions, input_ids, tokenizer, vocab):\n","    decoded_sentences = []\n","    for i, predicted_indices in enumerate(predictions):\n","        decoded_sentence = []\n","        for j, spelling_index in enumerate(predicted_indices):\n","            token = input_ids[i, j].item()\n","            if token == tokenizer.pad_token_id:\n","                continue\n","                \n","            sino_word = tokenizer.convert_ids_to_tokens(token)\n","            if spelling_index == -1:\n","                viet_spelling = vocab[sino_word][0]\n","            else:\n","                viet_spelling = vocab[sino_word][spelling_index]\n","            decoded_sentence.append(viet_spelling)\n","\n","        decoded_sentences.append(\" \".join(decoded_sentence))\n","    return decoded_sentences"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T15:16:07.590221Z","iopub.status.busy":"2024-06-05T15:16:07.589398Z","iopub.status.idle":"2024-06-05T15:16:07.612877Z","shell.execute_reply":"2024-06-05T15:16:07.611822Z","shell.execute_reply.started":"2024-06-05T15:16:07.590189Z"},"trusted":true},"outputs":[],"source":["def train(model, train_dataloader, test_dataloader, epochs=3, lr=1e-5, \n","          max_grad_norm=1.0, model_load_path=None, config_folder_dir=\"config/\"):\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","    model.to(device)\n","    tokenizer = model.tokenizer\n","\n","    optimizer = optim.AdamW(model.parameters(), lr=lr)\n","    criterion = torch.nn.CrossEntropyLoss(ignore_index=-1)\n","    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5)\n","    \n","    log_dir = os.path.join(config_folder_dir, f\"running/\")\n","    os.makedirs(log_dir, exist_ok=True)\n","    train_losses_dir = os.path.join(log_dir, f\"train_losses.txt\")\n","    test_losses_dir = os.path.join(log_dir, f\"test_losses.txt\")\n","    test_accuracies_dir = os.path.join(log_dir, f\"test_accuracies.txt\")\n","    test_wers_dir = os.path.join(log_dir, f\"test_wers.txt\")\n","    \n","    if model_load_path:\n","        model.load_state_dict(torch.load(model_load_path))\n","    # Determine the starting epoch\n","    start_epoch = 0\n","    if model_load_path:\n","        start_epoch = int(model_load_path.split(\"_\")[-1].split(\".\")[0]) \n","    \n","    for epoch in range(start_epoch, start_epoch + epochs):\n","        model.train()\n","        total_loss = 0\n","\n","        # Training loop with progress bar\n","        train_iterator = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{start_epoch + epochs}\", unit=\"batch\")\n","        for batch in train_iterator:\n","            input_ids = batch[\"input_ids\"].to(device)\n","            attention_mask = batch[\"attention_mask\"].to(device)\n","            labels = batch[\"labels\"].to(device)\n","\n","            optimizer.zero_grad()\n","\n","            outputs = model(input_ids, attention_mask=attention_mask)\n","\n","            # Flatten \n","            preds = outputs.view(-1, outputs.size(-1))\n","            targets = labels.view(-1)\n","\n","            loss = criterion(preds, targets) # batch loss\n","            total_loss += loss.item()\n","\n","            loss.backward()\n","            clip_grad_norm_(model.parameters(), max_grad_norm)\n","            optimizer.step()\n","            \n","            train_iterator.set_postfix(loss=loss.item())\n","\n","        avg_train_loss = total_loss / len(train_dataloader)\n","\n","        with open(train_losses_dir, 'a') as tl:\n","            tl.write(f\"{avg_train_loss};\")\n","\n","        print(f\"Epoch {epoch+1}/{start_epoch + epochs}, Training Loss: {avg_train_loss}\")\n","\n","        ################################## Run test ##################################\n","        model.eval()\n","        total_test_loss = 0\n","        correct_predictions = 0 # calculate accuracies over sino words that have multiple viet spellings only\n","        total_predictions = 0\n","        all_ground_truths = []\n","        all_predictions = []\n","        \n","        with torch.no_grad():\n","            test_iterator = tqdm(test_dataloader, desc=\"Validating\", unit=\"batch\")\n","            for batch in test_iterator:\n","                input_ids = batch[\"input_ids\"].to(device)\n","                attention_mask = batch[\"attention_mask\"].to(device)\n","                labels = batch[\"labels\"].to(device)\n","\n","                outputs = model(input_ids, attention_mask=attention_mask)\n","\n","                preds = outputs.view(-1, outputs.size(-1)) # Flatten\n","                targets = labels.view(-1)\n","\n","                test_loss = criterion(preds, targets)\n","                total_test_loss += test_loss.item()\n","\n","                predictions = torch.argmax(outputs, dim=-1)\n","                mask = labels != -1\n","                correct_predictions += (predictions[mask] == labels[mask]).sum().item()\n","                total_predictions += mask.sum().item()\n","                \n","                batch_predictions = decode_predictions(predictions, input_ids, tokenizer, model.vocab)\n","                batch_ground_truths = decode_predictions(labels, input_ids, tokenizer, model.vocab)\n","                all_predictions.extend(batch_predictions)\n","                all_ground_truths.extend(batch_ground_truths)\n","            \n","            avg_test_loss = total_test_loss / len(test_dataloader)\n","            with open(test_losses_dir, 'a') as tl2:\n","                tl2.write(f\"{avg_test_loss};\")\n","            \n","            test_accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n","            with open(test_accuracies_dir, 'a') as ta:\n","                ta.write(f\"{test_accuracy * 100};\")\n","            \n","            test_wer = wer(all_ground_truths, all_predictions)\n","            with open(test_wers_dir, 'a') as tw:\n","                tw.write(f\"{test_wer * 100};\")\n","            \n","            print(f\"Epoch {epoch+1}/{start_epoch + epochs}, Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_accuracy * 100:.4f}, Test WER: {test_wer * 100:.4f}\")\n","\n","        scheduler.step(avg_test_loss)\n","        print(f\"Learning rate: {scheduler.get_last_lr()}\")\n","        \n","        # Save the model after each epoch\n","        save_dir = os.path.join(log_dir, f\"saved_model/\")\n","        os.makedirs(save_dir, exist_ok=True)\n","        model_save_path = os.path.join(save_dir, f\"sivi_model_epoch_{epoch+1}.pt\")\n","        torch.save(model.state_dict(), model_save_path)\n","        print(f\"Model saved to {model_save_path}\")"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T15:09:18.450673Z","iopub.status.busy":"2024-06-05T15:09:18.450307Z","iopub.status.idle":"2024-06-05T15:09:19.629601Z","shell.execute_reply":"2024-06-05T15:09:19.628707Z","shell.execute_reply.started":"2024-06-05T15:09:18.450644Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["151.9M params.\n"]}],"source":["# Model config\n","hidden_ff_dim = config['model_config']['hidden_ff_dim']\n","model_hidden_dim = config['model_config']['model_hidden_dim']\n","shrink_norm_hidden = config['model_config']['shrink_norm_hidden']\n","large_hidden_classification_head_dim = config['model_config']['large_hidden_classification_head_dim']\n","small_hidden_classification_head_dim = config['model_config']['small_hidden_classification_head_dim']\n","max_num_spellings = config['model_config']['max_num_spellings']\n","num_spelling_threshold = config['model_config']['num_spelling_threshold']\n","train_bert_param = config['model_config']['train_bert_param']\n","dropout = config['model_config']['dropout']\n","\n","model = SinoVietnameseTranslator(base_tokenizer, base_model, base_vocab, hidden_ff_dim=hidden_ff_dim, \n","                                model_hidden_dim=model_hidden_dim, shrink_norm_hidden=shrink_norm_hidden,\n","                                large_hidden_classification_head_dim=large_hidden_classification_head_dim,\n","                                small_hidden_classification_head_dim=small_hidden_classification_head_dim,\n","                                max_num_spellings=max_num_spellings, train_bert_param=train_bert_param,\n","                                num_spelling_threshold=num_spelling_threshold, dropout=dropout)\n","\n","num_param = sum([param.nelement() for param in model.parameters()]) / 1000000\n","print(f\"{num_param:.1f}M params.\")"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T15:17:03.487090Z","iopub.status.busy":"2024-06-05T15:17:03.486391Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Epoch 1/60:   0%|          | 0/4793 [00:00<?, ?batch/s]c:\\Users\\ADMIN\\miniconda3\\envs\\dnn\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:435: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n","  attn_output = torch.nn.functional.scaled_dot_product_attention(\n","Epoch 1/60: 100%|██████████| 4793/4793 [1:31:25<00:00,  1.14s/batch, loss=0.856]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/60, Training Loss: 0.9126091286580448\n"]},{"name":"stderr","output_type":"stream","text":["Validating: 100%|██████████| 533/533 [10:19<00:00,  1.16s/batch]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/60, Test Loss: 0.8474, Test Accuracy: 91.3767, Test WER: 2.5048\n","Learning rate: [1e-05]\n","Model saved to conf\\saved_model/sivi_model_epoch_1.pt\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2/60: 100%|██████████| 4793/4793 [1:34:14<00:00,  1.18s/batch, loss=0.855]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2/60, Training Loss: 0.836351030683209\n"]},{"name":"stderr","output_type":"stream","text":["Validating: 100%|██████████| 533/533 [09:34<00:00,  1.08s/batch]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2/60, Test Loss: 0.8258, Test Accuracy: 92.3318, Test WER: 2.2274\n","Learning rate: [1e-05]\n","Model saved to conf\\saved_model/sivi_model_epoch_2.pt\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3/60:  21%|██        | 1005/4793 [19:12<1:17:34,  1.23s/batch, loss=0.843]"]}],"source":["# Trainning config\n","num_epochs = config['training_config']['num_epochs']\n","learning_rate = config['training_config']['learning_rate']\n","model_load_path = None if config['training_config']['model_load_path'] == 'None' else config['training_config']['model_load_path']\n","config_folder_dir = config_path\n","\n","train(model, train_loader, test_loader, epochs=num_epochs, lr=learning_rate,\n","    model_load_path=model_load_path, config_folder_dir=config_folder_dir)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T14:09:40.587214Z","iopub.status.busy":"2024-06-05T14:09:40.586806Z","iopub.status.idle":"2024-06-05T14:09:40.592014Z","shell.execute_reply":"2024-06-05T14:09:40.590974Z","shell.execute_reply.started":"2024-06-05T14:09:40.587183Z"},"trusted":true},"outputs":[],"source":["torch.cuda.empty_cache()"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5049399,"sourceId":8468706,"sourceType":"datasetVersion"},{"datasetId":5151403,"sourceId":8608725,"sourceType":"datasetVersion"}],"dockerImageVersionId":30716,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
